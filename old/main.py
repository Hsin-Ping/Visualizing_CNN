import numpy as npimport cv2import torchimport torch.nn as nnimport torchvision.transforms.v2 as transforms_v2import torchvision.transforms as transformsfrom models.VGG import vgg16_netfrom models.deconVGG import deconVgg16_net, deconVgg16_net_orifrom utils.utils import predictionfrom functools import partialdef register_net(net):     def hook(module, input, output, idx):        if isinstance(module, nn.Conv2d):            net.feature_maps[idx] = output        if isinstance(module, nn.MaxPool2d):            net.feature_maps[idx] = output[0]            net.pooling_loc[idx] = output[1]                    for idx, layer in enumerate(net.features):        layer.register_forward_hook(partial(hook, idx=idx))        def visualizing_feats(net, deconv_net):    feats_layers = [14, 17, 19, 21, 24, 26, 28]    for layer_idx in feats_layers:        feature_maps = net.feature_maps[layer_idx]        each_feats_max, _ = feature_maps.flatten(-2).max(-1)        best_feats_max, best_feats_idx = each_feats_max.max(-1)        best_feats = feature_maps[:,best_feats_idx,:,:]        condition = (best_feats==best_feats_max)        tsfm_best_feats = torch.where(condition, best_feats, torch.zeros(size=best_feats.size()))        tsfm_feature_maps = torch.zeros(size=feature_maps.size())        tsfm_feature_maps[:, best_feats_idx, :, :] = tsfm_best_feats                deconv_output = deconv_net(tsfm_feature_maps, layer_idx, net.pooling_loc)        #feats_img = result.permute(0, 2, 3, 1).view(224, 224, 3).detach().numpy()        #feats_img = (feats_img - feats_img.min()) / (feats_img.max() - feats_img.min()) * 255        #feats_img = feats_img.astype(np.uint8)        #print(feats_img.max())                feature_img = deconv_output.data.numpy()[0].transpose(1,2,0)        feature_img = (feature_img-feature_img.min()) / (feature_img.max()  - feature_img.min()) * 255        # have to add copy() to put text on image        feature_img = feature_img.astype(np.uint8).copy()        cv2.imshow("", feature_img)        cv2.waitKey(0)        cv2.destroyAllWindows()        def vis_layer(layer, vgg16conv, vgg16deconv, feature_maps, maxpooling_loc):        layer_feature_maps = feature_maps[layer]    print("layer_feature_maps",layer_feature_maps.size())        max_activations = []    for idx in range(0, layer_feature_maps.size()[1]):        feature_map = layer_feature_maps[:,idx,:,:]        max_activations.append(torch.max(feature_map))    max_activation = max(max_activations)    #print(max_activations)    choose_map_idx = max_activations.index(max_activation)    choose_map = layer_feature_maps[:, choose_map_idx, :,:]    #print(choose_map.size())    tsfm_layer_features = torch.zeros(layer_feature_maps.size())    #print(tsfm_layer_features.size())    tsfm_choose_map = torch.where(choose_map==max_activation, choose_map, torch.zeros(size=choose_map.size()))    tsfm_layer_features[:, choose_map_idx, :, :] = tsfm_choose_map    #print("tsfm_layer_features",tsfm_layer_features.size())    deconv_output = vgg16deconv(tsfm_layer_features, layer, vgg16conv.pooling_loc)        feature_img = deconv_output.data.numpy()[0].transpose(1,2,0)    feature_img = (feature_img-feature_img.min()) / (feature_img.max()  - feature_img.min()) * 255    # have to add copy() to put text on image    feature_img = feature_img.astype(np.uint8).copy()        return feature_imgif __name__ == "__main__":    """    img_path = "/Users/wangxinping/Desktop/github/Visualizing_CNN/data/cat.jpg"    img = cv2.imread(img_path)    img = cv2.resize(img, (224,224))    tsfm = transforms_v2.Compose([        transforms_v2.ToImage(),        transforms_v2.ToDtype(torch.float32, scale=True)        ])    img = tsfm(img)    #print(img)    # add 1 more dimension represent batch size    img = img.view(1, 3, 224, 224)    prediction(img, net)    obj = net(img)    """        image_path = "./data/cat.jpg"        img_path = "/Users/wangxinping/Desktop/github/Visualizing_CNN/data/cat.jpg"    img = cv2.imread(img_path)    resize_img = cv2.resize(img, (224,224))    tsfm = transforms_v2.Compose([      transforms_v2.ToImage(),      transforms_v2.ToDtype(torch.float32, scale=True)      ])    img = tsfm(resize_img)    img = img.view(1, 3, 224, 224)        net = vgg16_net()    register_net(net)        prob = net(img)    deconv_net = deconVgg16_net()    visualizing_feats(net, deconv_net)        #vgg16deconv = vgg16Deconv()    """    vis_layers = [14, 17, 19, 21, 24, 26, 28]        all_images = resize_img.copy()        for idx, layer in enumerate(vis_layers):        print("idx",idx)        feature_img = vis_layer(layer, net, deconv_net, net.feature_maps, net.pooling_loc)        cv2.putText(feature_img, f"layer{layer}", (160,210), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0, 255), thickness=2)        if idx == 3:            col2 = feature_img        elif idx > 3:            col2 = np.hstack((col2, feature_img))        else:            all_images = np.hstack((all_images,feature_img))    all_images = np.vstack((all_images, col2))                cv2.imshow("feature_img", all_images)    cv2.waitKey(0)    cv2.destroyAllWindows()    """